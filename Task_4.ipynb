{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task-4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhu4nTHX+kVMz9pGW5O/B4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prantik-pdeb/GSoC2022-QML/blob/main/Task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task IV: Classical Graph Neural Network (GNN) Part"
      ],
      "metadata": {
        "id": "4UbtrpHQUmwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ParticleNet’s data for Quark/Gluon jet classification available [here](https://www.google.com/url?q=https%3A%2F%2Fzenodo.org%2Frecord%2F3164691%23.YigdGt9MHrB&sa=D&source=docs) with its corresponding description.\n",
        "\n",
        "● 2 Graph-based architectures Graph Attention Network(GAT) and Graph Convulational Network(GCN) to classify jets as being quarks or gluons using Pytorch and Deep Graph Library.\n"
      ],
      "metadata": {
        "id": "fPaq4U2iUrAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Information"
      ],
      "metadata": {
        "id": "vzDmQTLuWEZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X: (100000,M,4), exactly 50k quark and 50k gluon jets, randomly sorted, where M is the max multiplicity of the jets in that file (other jets have been padded with zero-particles), and the features are:\n",
        "\n",
        "a) pt, \n",
        "\n",
        "b) rapidity, \n",
        "\n",
        "c) azimuthal angle, and \n",
        "\n",
        "d) pdgid (particle data group ID)\n",
        "\n",
        "\n",
        "y: (100000,), an array of labels for the jets where gluon = 0 and quark = 1."
      ],
      "metadata": {
        "id": "-pon871cV6dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install graph-based library\n",
        "!pip install dgl"
      ],
      "metadata": {
        "id": "gRA9rxD1Un_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import torch\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import torch.nn as nn\n",
        "import dgl.nn as dglnn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dgl.data.utils import save_graphs, load_graphs\n",
        "from dgl.nn.pytorch.conv import GraphConv\n",
        "\n",
        "# Visualization modules\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3sIECjSPWXVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Ys6EcJ05Wppd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dict(np.load('/content/gdrive/MyDrive/QG_jets.npz'))\n",
        "X, y = dataset['X'], {'label': torch.tensor(dataset['y']).long()}"
      ],
      "metadata": {
        "id": "sK-LbszmWtlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "yFVCMMseWywI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph representation"
      ],
      "metadata": {
        "id": "BWZd_GVDeAIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Mathematically, a graph $\\mathcal{G}$ is defined as a tuple of a set of nodes/vertices $V$, and a set of edges/links $E$: $\\mathcal{G}=(V,E)$.\n",
        "Each edge is a pair of two vertices, and represents a connection between them.\n",
        "For instance, let's look at the following graph:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If we have edge attributes or different categories of edges in a graph, this information can be added to the matrix as well.\n",
        "For an undirected graph, keep in mind that $A$ is a symmetric matrix ($A_{ij}=A_{ji}$)."
      ],
      "metadata": {
        "id": "xHeBwAjBeT63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph can have features in three different levels:\n",
        "\n",
        "a) features for nodes\n",
        "b) features for edges\n",
        "c) features for the whole graph\n",
        "\n",
        "jet is encoded as a graph. And the binary-label of whether it is a quark or a gluon is encoded as the graph feature."
      ],
      "metadata": {
        "id": "BmvgOEZaeDYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Geometric"
      ],
      "metadata": {
        "id": "mCNzu2gfg2hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing graph networks with adjacency matrix is simple and straight-forward but can be computationally expensive for large graphs. Many real-world graphs can reach over 200k nodes, for which adjacency matrix-based implementations fail. There are a lot of optimizations possible when implementing GNNs, and luckily, there exist packages that provide such layers. The most popular packages for PyTorch are [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) and the [Deep Graph Library](https://www.dgl.ai/)"
      ],
      "metadata": {
        "id": "gr_8_5axglFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import networkx as nx\n",
        "from dgl.data.utils import save_graphs, load_graphs\n",
        "\n",
        "# https://github.com/dmlc/xgboost/issues/1715\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "class GenerateGraphs(object):\n",
        "    def describe(self): return self.__class__.__name__\n",
        "    def __init__(self, data, connect_style):\n",
        "        '''\n",
        "          Args:\n",
        "            data: np.ndarry\n",
        "              input data\n",
        "            connect_style: str\n",
        "              Type of edge connections\n",
        "        '''\n",
        "        self.data = data\n",
        "        self.label_name = 'label'\n",
        "        self.feature_name = 'feature'\n",
        "        self.connect_style = connect_style\n",
        "        self._current_data = None\n",
        "\n",
        "    def _assign_node_feature(self, graph):\n",
        "        ''' Each node presents a b-jet with four momentum and b-tagging score as features.\n",
        "        '''\n",
        "        feature_name = 'feature'\n",
        "        assert(self._current_data is not None)\n",
        "        graph.ndata[feature_name] = torch.tensor(self._current_data)\n",
        "        return graph\n",
        "\n",
        "    def _create_graph(self, ievent: int):\n",
        "        ''' Create a graph from a jet X [track multiplicity, \"4-momentum\"], where \"4-momentum\" is pt, rapidity, azimuthal angle, and pdgid.\n",
        "            data: https://zenodo.org/record/3164691#.YFeQey1Q0lp\n",
        "        '''\n",
        "        self._current_data = self.data[ievent][~np.all(self.data[ievent] == 0, axis=1)]\n",
        "\n",
        "        ''' Feature preprocessing: Sect 3.1 in https://arxiv.org/pdf/1810.05165.pdf\n",
        "            centering jets and normalizing pT\n",
        "            https://energyflow.network/examples/\n",
        "        '''\n",
        "        yphi_avg = np.average(self._current_data[:,1:3], weights=self._current_data[:,0], axis=0)\n",
        "        self._current_data[:,1:3] -= yphi_avg\n",
        "        self._current_data[:, 0] /= np.sum(self._current_data[:, 0])\n",
        "\n",
        "        ''' - Sort by pT (0th column) '''\n",
        "        self._current_data = self._current_data[self._current_data[:,0].argsort()][::-1].copy()\n",
        "\n",
        "        ''' Construct a graph '''\n",
        "        n_nodes = self._current_data.shape[0] # track multiplicity\n",
        "        if self.connect_style == 'bifully':\n",
        "            ''' Option 1: Fully connected graph '''\n",
        "            g = nx.complete_graph(n_nodes)\n",
        "            graph = dgl.from_networkx(g)\n",
        "        else:\n",
        "            ''' Option 2: Bi-directional connection in adjacent track in pT or eta or phi'''\n",
        "            pt_order = self._current_data[:,0].argsort()[::-1]\n",
        "            rapidity_order = self._current_data[:,1].argsort()[::-1]\n",
        "            eta_order = self._current_data[:,2].argsort()[::-1]\n",
        "\n",
        "            if self.connect_style == 'biadj_pt_y_phi':\n",
        "                in_node  = np.concatenate((pt_order[:-1], rapidity_order[:-1], eta_order[:-1]))\n",
        "                out_node = np.concatenate((pt_order[1: ], rapidity_order[1: ], eta_order[1: ]))\n",
        "            elif self.connect_style == 'biadj_pt_y':\n",
        "                in_node  = np.concatenate((pt_order[:-1], rapidity_order[:-1]))\n",
        "                out_node = np.concatenate((pt_order[1: ], rapidity_order[1: ]))\n",
        "            elif self.connect_style == 'biadj_pt_phi':\n",
        "                in_node  = np.concatenate((pt_order[:-1], eta_order[:-1]))\n",
        "                out_node = np.concatenate((pt_order[1: ], eta_order[1: ]))\n",
        "            elif self.connect_style == 'biadj_y_phi':\n",
        "                in_node  = np.concatenate((rapidity_order[:-1], eta_order[:-1]))\n",
        "                out_node = np.concatenate((rapidity_order[1: ], eta_order[1: ]))\n",
        "\n",
        "            g = dgl.graph(( in_node, out_node), num_nodes=n_nodes)\n",
        "            g = dgl.add_reverse_edges(g)\n",
        "            graph = dgl.add_self_loop(g)\n",
        "\n",
        "\n",
        "        ''' Assign node feature using \"current data\" '''\n",
        "        graph = self._assign_node_feature(graph)\n",
        "\n",
        "        return graph.int() # 32-bit integers for node and edge IDs to reduce memory\n",
        "\n",
        "    def create_graphs(self, stop=None):\n",
        "        ''' Create all graphs for all events.\n",
        "        '''\n",
        "\n",
        "        ''' PDGid to small float dictionary https://github.com/pkomiske/EnergyFlow/blob/master/energyflow/utils/data_utils.py#L188 '''\n",
        "        PID2FLOAT_MAP = {22: 0,\n",
        "                    211: .1, -211: .2,\n",
        "                    321: .3, -321: .4,\n",
        "                    130: .5,\n",
        "                    2112: .6, -2112: .7,\n",
        "                    2212: .8, -2212: .9,\n",
        "                    11: 1.0, -11: 1.1,\n",
        "                    13: 1.2, -13: 1.3,\n",
        "                    0: 0,}\n",
        "        for pid in np.unique(self.data[:, :, 3].flatten()):\n",
        "            np.place(self.data[:, :, 3], self.data[:, :, 3] == pid, PID2FLOAT_MAP[pid])\n",
        "\n",
        "        graphs = []\n",
        "        n_graphs = min(stop, self.data.shape[0]) if stop else self.data.shape[0]\n",
        "        \n",
        "        for i in range(n_graphs):\n",
        "            if i % 1000 == 0:\n",
        "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', f'\\033[92mCreated {self.connect_style} graphs:\\033[0m'.rjust(40, ' '),  i, '/', n_graphs)\n",
        "            graph = self._create_graph(i)\n",
        "            graphs.append(graph)\n",
        "        return graphs"
      ],
      "metadata": {
        "id": "z6mrMr4OW6e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting the nodes if they are adjacent\n",
        "generator = GenerateGraphs(X, connect_style='biadj_pt_y_phi')\n",
        "graphs = generator.create_graphs()\n",
        "graph_file_name = 'QG_jets_{connection}.bin'\n",
        "save_graphs(graph_file_name, graphs, y)"
      ],
      "metadata": {
        "id": "2eTwwV9VXE5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(graph):\n",
        "    print('Number of nodes and edges: {} / {}'.format(graph.number_of_nodes(), graph.number_of_edges()))\n",
        "    nx.draw(graph.to_networkx(), with_labels=True, node_color=[[.5, .5, .5]])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1f842Ms1XJw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph visualization example-1\n",
        "plot_graph(graphs[0])"
      ],
      "metadata": {
        "id": "A4f5fOwUXMeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph visualization example-2\n",
        "plot_graph(graphs[1])"
      ],
      "metadata": {
        "id": "cYHi0VtpXYSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preapring dataset train, test and validation split"
      ],
      "metadata": {
        "id": "jV7bBwKWfmeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here spliting ratio is:\n",
        "\n",
        "train : test : validation = 7:2:1 with batch size = 1."
      ],
      "metadata": {
        "id": "_XrwGftYfxsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset train, test valodation split\n",
        "train_size = int(len(graphs) * 0.7)\n",
        "val_size = int(len(graphs) * 0.2)\n",
        "test_size = len(graphs) - train_size - val_size\n",
        "\n",
        "dataset = list(zip(graphs, y['label']))\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, val_size, test_size))\n",
        "\n",
        "#batch size =1\n",
        "train_dataloader = dgl.dataloading.GraphDataLoader(train_dataset, batch_size = train_size, drop_last=False, shuffle=True)\n",
        "val_dataloader = dgl.dataloading.GraphDataLoader(val_dataset, batch_size = val_size, drop_last=False, shuffle=False)\n",
        "test_dataloader = dgl.dataloading.GraphDataLoader(test_dataset, batch_size = test_size, drop_last=False, shuffle=False)\n",
        "\n",
        "print('Train size: ', train_size)\n",
        "print('Validation size: ', val_size)\n",
        "print('Test size: ', test_size)"
      ],
      "metadata": {
        "id": "gpxxehC7Xai3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Neural Network(GNN)"
      ],
      "metadata": {
        "id": "IqcDY9AwhTOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN when we really mean a layer from a GNN. Most GNNs implement a specific layer that can deal with graphs, and so usually we are only concerned with this layer. An example of a simple layer for a GNN:\n",
        "\n",
        "\\begin{equation}\n",
        "f_k = \\sigma\\left( \\sum_i \\sum_j v_{ij}w_{jk}  \\right)\n",
        "\\end{equation}\n",
        "\n",
        "This equation shows that we first multiply every node ($v_{ij}$) feature by trainable weights $w_{jk}$, sum over all node features, and then apply an activation. This will yield a single feature vector for the graph. This is equation permutation equivariant because the node index in our expression is index $i$ which can be re-ordered without affecting the output."
      ],
      "metadata": {
        "id": "z8gDvBnXhUUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNmodel(nn.Module):\n",
        "    def __init__(self, module, in_features, hidden_features, out_features, name='GNN'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.name = name\n",
        "\n",
        "        if module.lower() == 'sage':\n",
        "            # from dgl.nn import SAGEConv\n",
        "            self.module = SAGE(in_features, hidden_features, out_features)\n",
        "        elif module.lower() == 'gat':\n",
        "            # from dgl.nn import GATConv\n",
        "            self.module = GAT(in_features, hidden_features, out_features)\n",
        "        elif module.lower() == 'agnnconv':\n",
        "            # from dgl.nn import AGNNConv\n",
        "            self.module = AGNNConv()\n",
        "        elif module.lower() == 'sgc':\n",
        "            # from dgl.nn import SGConv\n",
        "            self.module = SGC(in_features, hidden_features, out_features)\n",
        "        elif module.lower() == 'gcn':\n",
        "            self.module = GCN(in_features, hidden_features, out_features)\n",
        "        else:\n",
        "            assert False, module + ' is not supported'\n",
        "\n",
        "        # self.pred = MLPPredictor(out_features, 1)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        h = self.module(g, x)\n",
        "        return torch.squeeze(h) #self.pred(g, h)"
      ],
      "metadata": {
        "id": "wY5bwR2iX0_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Network(GAT)"
      ],
      "metadata": {
        "id": "5x4IySVVX_cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Attention**](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html#What-is-Attention?) concept can be similarly applied to graphs, one of such is the Graph Attention Network called GAT, proposed by Velickovic et al., 2017. \n",
        "\n",
        "The graph attention layer creates a message for each node using a linear layer/weight matrix. For the attention part, it uses the message from the node itself as a query, and the messages to average as both keys and values (note that this also includes the message to itself). The score function $f_{attn}$ is implemented as a one-layer MLP which maps the query and key to a single value.\n",
        "\n",
        "$h_i$ and $h_j$ are the original features from node $i$ and $j$ respectively, and represent the messages of the layer with $\\mathbf{W}$ as weight matrix. $\\mathbf{a}$ is the weight matrix of the MLP, which has the shape $[1,2\\times d_{\\text{message}}]$, and $\\alpha_{ij}$ the final attention weight from node $i$ to $j$. The calculation can be described as follows:\n",
        "\n",
        "$$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)\\right)}$$\n",
        "\n",
        "The operator $||$ represents the concatenation, and $\\mathcal{N}_i$ the indices of the neighbors of node $i$.\n",
        "\n",
        "We can see that without the non-linearity, the attention term with $h_i$ actually cancels itself out, resulting in the attention being independent of the node itself. Hence, we would have the same issue as the GCN of creating the same output features for nodes with the same neighbors. This is why the LeakyReLU is crucial and adds some dependency on $h_i$ to the attention. \n",
        "\n",
        "Once we obtain all attention factors, we can calculate the output features for each node by performing the weighted average:\n",
        "\n",
        "$$h_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}h_j\\right)$$\n",
        "\n",
        "$\\sigma$ is another non-linearity, as in the GCN layer."
      ],
      "metadata": {
        "id": "xhQzlsPZX9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "    ''' Two layers of Graph Attention Network.\n",
        "        https://docs.dgl.ai/api/python/nn.pytorch.html#gatconv\n",
        "    '''\n",
        "    def __init__(self, in_feats, hid_feats, out_feats):\n",
        "        super().__init__()\n",
        "        # input shape = (nodes, features=in_feats); output shape = (nodes, num_head, hid_feats)\n",
        "        self.gatconv1 = dglnn.GATConv(in_feats, hid_feats, num_heads=2)\n",
        "        # input shape = (nodes, hid_feats * num_heads_previous_layer); output shape = (nodes, num_head, out_feats)\n",
        "        self.gatconv2 = dglnn.GATConv(hid_feats * 2, out_feats, num_heads=1)\n",
        "\n",
        "    def forward(self, graph, inputs):\n",
        "        # input shape = (nodes , features)\n",
        "        # print('inputs', inputs.shape, inputs)\n",
        "        h = self.gatconv1(graph, inputs)\n",
        "        # here h shape = (nodes, num_head, hid_feats)\n",
        "        # print('h1', h.shape, h)\n",
        "        ''' Reshape h to flatten the num_heads '''\n",
        "        # here h shape = (nodes, num_head * hid_feats)\n",
        "        h = h.reshape(h.shape[0], np.prod(h.shape[1:]))\n",
        "        # print('h2', h.shape, h)\n",
        "        h = self.gatconv2(graph, h)\n",
        "        # print('h3', h.shape, h)\n",
        "\n",
        "        graph.ndata['tmp_feature'] = h\n",
        "        h = dgl.mean_nodes(graph, 'tmp_feature')\n",
        "        # print('h4', h.shape, h)\n",
        "        h = torch.sigmoid(h)\n",
        "        h = torch.squeeze(h)\n",
        "        # print('h9', h.shape, h)\n",
        "        return h\n",
        "\n",
        "class AGNNConv(nn.Module):\n",
        "    ''' Graph Attention Network.\n",
        "        https://docs.dgl.ai/api/python/nn.pytorch.html#agnnconv\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, inputs):\n",
        "        # inputs are features of nodes\n",
        "        h = AGNNConv()(graph, inputs)\n",
        "        h = torch.sigmoid(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "qxyB7gkAYDiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Convolution Network(GCN)"
      ],
      "metadata": {
        "id": "QXGrXDh0YRXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Convolutional Networks(GCN) have been introduced by Kipf et al.\n",
        "in 2016 at the University of Amsterdam.\n",
        "\n",
        "GCNs are similar to convolutions in images in the sense that the \"filter\" parameters are typically shared over all locations in the graph.\n",
        "At the same time, GCNs rely on message passing methods, which means that vertices exchange information with the neighbors,\n",
        "and send \"messages\" to each other.\n",
        "\n",
        "\n",
        "As the number of messages vary across nodes, we need an operation that works for any number.\n",
        "Hence, the usual way to go is to sum or take the mean.\n",
        "Given the previous features of nodes $H^{(l)}$, the GCN layer is defined as follows:\n",
        "\n",
        "$$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$$\n",
        "\n",
        "$W^{(l)}$ is the weight parameters with which we transform the input features into messages ($H^{(l)}W^{(l)}$).\n",
        "To the adjacency matrix $A$ we add the identity matrix so that each node sends its own message also to itself:\n",
        "$\\hat{A}=A+I$.\n",
        "Finally, to take the average instead of summing, we calculate the matrix $\\hat{D}$ which is a diagonal\n",
        "matrix with $D_{ii}$ denoting the number of neighbors node $i$ has.\n",
        "$\\sigma$ represents an arbitrary activation function, and not necessarily the sigmoid (usually a ReLU-based\n",
        "activation function is used in GNNs)."
      ],
      "metadata": {
        "id": "CFtKdHKXYO9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    ''' Graph Convolutional Network\n",
        "        https://docs.dgl.ai/en/0.4.x/tutorials/basics/4_batch.html#graph-classification-tutorial\n",
        "        https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/1_gcn.html\n",
        "    '''\n",
        "    def __init__(self, in_dim, hidden_dim, out_feats):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
        "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GraphConv(hidden_dim, out_feats)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=hidden_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=out_feats)\n",
        "\n",
        "    def forward(self, graph, h):\n",
        "        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "        # is the same as the out_degree.\n",
        "        # h = graph.in_degrees().view(-1, 1).float()\n",
        "        # Perform graph convolution and activation function.\n",
        "        h = F.relu(self.bn1(self.conv1(graph, h)))\n",
        "        h = F.relu(self.bn2(self.conv2(graph, h)))\n",
        "        h = F.relu(self.bn3(self.conv3(graph, h)))\n",
        "        graph.ndata['tmp_feature'] = h\n",
        "        h = dgl.mean_nodes(graph, 'tmp_feature')\n",
        "        # Calculate graph representation by averaging all the node representations.\n",
        "        h = torch.sigmoid(h)\n",
        "        h = (h-0.5) * 2\n",
        "        return h"
      ],
      "metadata": {
        "id": "7WE3tojcYa9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "9zhReCfIYdI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training loop\n",
        "def train(model, optimizer, epochs=100, loss_func=nn.MSELoss()):\n",
        "  epoch_losses = {'train': [], 'val': []}\n",
        "  for epoch in range(epochs):\n",
        "      train_loss = 0\n",
        "      for ibatch, (batched_graph, labels) in enumerate(train_dataloader):\n",
        "          node_features = batched_graph.ndata['feature']\n",
        "          pred = model(batched_graph, node_features.float())\n",
        "          loss = loss_func(pred, labels.float())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.detach().item()\n",
        "\n",
        "      # The loss per epoch for all batch is the average of losses per batch in this epoch \n",
        "      train_loss /= (ibatch + 1)\n",
        "\n",
        "      # Store loss per epoch \n",
        "      epoch_losses['train'].append(train_loss)\n",
        "\n",
        "      # Evaluate validation loss\n",
        "      for ibatch, (batched_graph, labels) in enumerate(val_dataloader):\n",
        "          node_features = batched_graph.ndata['feature']\n",
        "          pred = model(batched_graph, node_features.float())\n",
        "          val_loss = loss_func(pred, labels.float()).detach().item()\n",
        "          epoch_losses['val'].append(val_loss)\n",
        "          assert(ibatch == 0)\n",
        "\n",
        "      print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
        "\n",
        "      # Early stopping\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=False, path='{}.checkpoint'.format(model.name))\n",
        "      # early_stopping needs the validation loss to check if it is decreasing, \n",
        "      # and if it is, it will make a checkpoint of the current model\n",
        "      early_stopping(val_loss, model)\n",
        "\n",
        "      if early_stopping.early_stop:\n",
        "          print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
        "          print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mEarly stop at:\\033[0m'.rjust(40, ' '),  epoch)\n",
        "          break\n",
        "  return epoch_losses"
      ],
      "metadata": {
        "id": "B1SVTMdeYgnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # define test loop\n",
        " def test(model, loss_func=nn.MSELoss()):\n",
        "    epoch_losses = {'test': []}\n",
        "    with torch.no_grad():\n",
        "        ''' Final Evaluate train loss '''\n",
        "        train_pred = []\n",
        "        train_labels = []\n",
        "        for ibatch, (batched_graph, labels) in enumerate(train_dataloader):\n",
        "            node_features = batched_graph.ndata['feature']\n",
        "            pred = model(batched_graph, node_features.float()).detach()\n",
        "            # print('zhangr train pred', pred.shape, pred)\n",
        "            # print('zhangr train labels', labels.shape, labels)\n",
        "            train_pred.append(pred)\n",
        "            train_labels.append(labels)\n",
        "        train_pred = torch.cat(train_pred).numpy()\n",
        "        train_labels = torch.cat(train_labels).numpy()\n",
        "\n",
        "        ''' Final Evaluate val loss '''\n",
        "        val_pred = []\n",
        "        val_labels = []\n",
        "        for ibatch, (batched_graph, labels) in enumerate(train_dataloader):\n",
        "            node_features = batched_graph.ndata['feature']\n",
        "            pred = model(batched_graph, node_features.float()).detach()\n",
        "            val_pred.append(pred)\n",
        "            val_labels.append(labels)\n",
        "            assert(ibatch == 0)\n",
        "        val_pred = torch.cat(val_pred).numpy()\n",
        "        val_labels = torch.cat(val_labels).numpy()\n",
        "\n",
        "\n",
        "        ''' Evaluate test loss '''\n",
        "        test_pred = []\n",
        "        test_labels = []\n",
        "        for ibatch, (batched_graph, labels) in enumerate(test_dataloader):\n",
        "            node_features = batched_graph.ndata['feature']\n",
        "            pred = model(batched_graph, node_features.float()).detach()\n",
        "            test_loss = loss_func(pred, labels.float()).detach().item()\n",
        "            epoch_losses['test'].append(test_loss)\n",
        "            test_pred.append(pred)\n",
        "            test_labels.append(labels)\n",
        "            assert(ibatch == 0)\n",
        "        test_pred = torch.cat(test_pred).numpy()\n",
        "        test_labels = torch.cat(test_labels).numpy()\n",
        "    return epoch_losses, train_pred, train_labels, val_pred, val_labels, test_pred, test_labels"
      ],
      "metadata": {
        "id": "5QBcWbzIkYnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAT - Implemenatuions and Result"
      ],
      "metadata": {
        "id": "T_jg3B_wjctW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_features, out_features = 9, 1 # 2 = classifier predict\n",
        "torch.manual_seed(0)\n",
        "# create the model\n",
        "gat_model = GNNmodel('GAT', graphs[0].ndata[list(graphs[0].ndata.keys())[0]].shape[1], hidden_features, out_features, name='GAT')\n",
        "print('Node feature dimension evolution {}->{}->{}'.format(graphs[0].ndata[list(graphs[0].ndata.keys())[0]].shape[1], hidden_features, out_features))"
      ],
      "metadata": {
        "id": "ojzFdGxdYiWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "-1Y-OPtxYj-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = train(gat_model, optimizer)"
      ],
      "metadata": {
        "id": "kfaFsv1MYne_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the last checkpoint with the best model.\n",
        "gat_model.load_state_dict(torch.load(gat_model.name+'.checkpoint'))\n",
        "# save the model\n",
        "torch.save(gat_model, gat_model.name)"
      ],
      "metadata": {
        "id": "Zpizina2kI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses, train_pred, train_labels, val_pred, val_labels, test_pred, test_labels = test(gat_model)"
      ],
      "metadata": {
        "id": "qkTd_b8ZkN0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss over time\n",
        "def plot_loss(*losses, labels=None):\n",
        "    plt.clf()\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    for loss in losses:\n",
        "        plt.plot(loss)\n",
        "    plt.ylabel('Loss', fontsize=15)\n",
        "    plt.xlabel('Epoch', fontsize=15)\n",
        "    if labels:\n",
        "      plt.legend(labels, loc='upper right', fontsize=15)\n",
        "    return plt\n",
        "\n",
        "\n",
        "plot_loss(losses['train'], losses['val'], labels=['train', 'val'])"
      ],
      "metadata": {
        "id": "iNi6_SDPkpg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FJTtW9hzkvhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN - Implemenatuions and Result"
      ],
      "metadata": {
        "id": "FFy2gOQsk08l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_features, out_features = 9, 1 # 2 = classifier predict\n",
        "torch.manual_seed(0)\n",
        "# create the model\n",
        "gcn_model = GNNmodel('GCN', graphs[0].ndata[list(graphs[0].ndata.keys())[0]].shape[1], hidden_features, out_features, name='GAT')\n",
        "print('Node feature dimension evolution {}->{}->{}'.format(graphs[0].ndata[list(graphs[0].ndata.keys())[0]].shape[1], hidden_features, out_features))"
      ],
      "metadata": {
        "id": "pGXD0Ecsk7qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "xlcdWZIQlHGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_2 = train(gcn_model, optimizer)"
      ],
      "metadata": {
        "id": "WR1ndbYTlJHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the last checkpoint with the best model.\n",
        "gcn_model.load_state_dict(torch.load(gcn_model.name+'.checkpoint'))\n",
        "# save the model\n",
        "torch.save(gcn_model, gcn_model.name)"
      ],
      "metadata": {
        "id": "EvKrC_l3lLTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses2, train_pred2, train_labels2, val_pred2, val_labels2, test_pred2, test_labels2 = test(gcn_model)"
      ],
      "metadata": {
        "id": "VLTup6AJlN3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(losses_2['train'], losses_2['val'], labels=['train', 'val'])"
      ],
      "metadata": {
        "id": "XGxgu7HelPkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am using two most popular Graph based architectures:-\n",
        "\n",
        "\n",
        "\n",
        "1.   Graph Attention Network(GAT)\n",
        "2.   Graph Convolution Network(GCN)\n",
        "\n"
      ],
      "metadata": {
        "id": "GVTKpuU6lu1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference"
      ],
      "metadata": {
        "id": "zN4r6ZWElWWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., &amp; Bengio, Y. (2018, February 4). Graph attention networks. arXiv.org. Retrieved April 19, 2022, from https://arxiv.org/abs/1710.10903 \n",
        "\n",
        "2. Kipf, T. N., &amp; Welling, M. (2017, February 22). Semi-supervised classification with graph Convolutional Networks. arXiv.org. Retrieved April 19, 2022, from https://arxiv.org/abs/1609.02907 \n",
        "\n",
        "3. Berg, R. van den, Kipf, T. N., &amp; Welling, M. (2017, October 25). Graph convolutional matrix completion. arXiv.org. Retrieved April 19, 2022, from https://arxiv.org/abs/1706.02263 \n",
        "\n",
        "4. Graph neural networks: A review of methods and applications. (n.d.). Retrieved April 19, 2022, from https://arxiv.org/ftp/arxiv/papers/1812/1812.08434.pdf \n",
        "\n",
        "5. Cohen, T., &amp; Welling, M. (2016, June 11). Group equivariant Convolutional Networks. PMLR. Retrieved April 19, 2022, from http://proceedings.mlr.press/v48/cohenc16.html \n",
        "\n",
        "\n",
        "6. Machine Learning with Graphs | Stanford Online. CS224W | Home. (n.d.). Retrieved April 19, 2022, from http://web.stanford.edu/class/cs224w/ \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A0NQdTqpocRh"
      }
    }
  ]
}